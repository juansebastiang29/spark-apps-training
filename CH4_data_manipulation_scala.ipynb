{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.{SparkContext, SparkConf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conf = org.apache.spark.SparkConf@51870c49\n",
       "Spark = org.apache.spark.SparkContext@1450ce70\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@1450ce70"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf = new SparkConf()\n",
    "                .setAppName(\"SparkLearning_Charper_4\")\n",
    "                .set(\"spark.driver.memory\",\"16g\")\n",
    "                    \n",
    "val Spark = new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"True\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(\"Spark-The-Definitive-Guide/data/retail-data/all/online-retail-dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col(\"InvoiceNo\").equalTo(536365))\n",
    "    .select(\"InvoiceNo\", \"Description\")\n",
    "    .show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val priceFilter = col(\"UnitPrice\") > 600\n",
    "val descripFilter = col(\"Description\").contains(\"POSTAGE\")\n",
    "\n",
    "df.where(col(\"StockCode\").isin(\"DOT\"))\n",
    "        .where(priceFilter.or(descripFilter))\n",
    "        .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " // Adding a boolean column to a df based on logical conditions\n",
    "val DOTCodeFilter = col(\"StockCode\") === \"DOT\"\n",
    "val priceFilter = col(\"UnitPrice\") > 600\n",
    "val descripFilter = col(\"Description\").contains(\"POSTAGE\")\n",
    "df.withColumn(\"isExpensive\",\n",
    "              DOTCodeFilter.and(priceFilter.and(descripFilter)))\n",
    "    .where(\"isExpensive\")\n",
    "//     .select(\"unitPrice\", \"isExpensive\")\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"),2)\n",
    "df.select(expr(\"CustomerId\"),\n",
    "        fabricatedQuantity.alias(\"realQuantity\"))\n",
    "        .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fabricatedQuantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\n",
    "\"CustomerId\",\n",
    "\"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\")\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(corr(\"Quantity\", \"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().select(col(\"summary\"),col(\"Quantity\"),col(\"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// # More stats functions\n",
    "val colName = \"UnitPrice\"\n",
    "val quantileProbs = Array(0.25,0.5,0.75,0.95)\n",
    "val relError = 0.05\n",
    "df.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stat.freqItems(Seq(\"StockCode\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// import org.apache.spark.sql.functions.{initcap}\n",
    "df.select(initcap(col(\"Description\"))).show(2, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "col(\"Description\"),\n",
    "lower(col(\"Description\")),\n",
    "upper(lower(col(\"Description\")))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Another trivial task is adding or removing whitespace around a string. We can\n",
    "// do this with lpad , ltrim , rpad and rtrim , trim .\n",
    "\n",
    "df.select(\n",
    "ltrim(lit(\" HELLO \")).as(\"ltrim\"),\n",
    "rtrim(lit(\" HELLO    \")).as(\"rtrim\"),\n",
    "trim(lit(\"   HELLO    \")).as(\"trim\"),\n",
    "lpad(lit(\"HELLO\"), 3, \" \").as(\"lp\"),\n",
    "rpad(lit(\"HELLO\"), 10, \" \").as(\"rp\"))\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.regexp_replace\n",
    "\n",
    "val simpleColors = Seq(\"black\", \"white\", \"red\", \"green\", \"blue\")\n",
    "val regexString = simpleColors.map(_.toUpperCase).mkString(\"|\")\n",
    "// the | signifies `OR` in regular expression syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "regexp_replace(col(\"Description\"), regexString, \"COLOR\")\n",
    ".alias(\"color_cleaned\"),\n",
    "col(\"Description\"))\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "translate(col(\"Description\"), \"LEET\", \"1337\"),\n",
    "col(\"Description\"))\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.regexp_extract\n",
    "val regexString = simpleColors.map(_.toUpperCase).mkString(\"(\", \"|\", \")\")\n",
    "// the | signifies OR in regular expression syntax\n",
    "df.select(\n",
    "        regexp_extract(col(\"Description\"), regexString, 1)\n",
    "        .alias(\"color_cleaned\"),\n",
    "        col(\"Description\"))\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val containsBlack = col(\"Description\").contains(\"BLACK\")\n",
    "val containsWhite = col(\"DESCRIPTION\").contains(\"WHITE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"hasSimpleColor\", containsBlack.or(containsWhite))\n",
    ".filter(\"hasSimpleColor\")\n",
    ".select(col(\"Description\"),col(\"hasSimpleColor\"))\n",
    ".show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val simpleColors = Seq(\"black\", \"white\", \"red\", \"green\", \"blue\")\n",
    "val selectedColumns = simpleColors.map(color => {\n",
    "                                                col(\"Description\")\n",
    "                                                .contains(color.toUpperCase)\n",
    "                                                .alias(s\"is_$color\")\n",
    "                                                }):+expr(\"*\") // could also append this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(selectedColumns:_*)\n",
    "        .where(col(\"is_white\").or(col(\"is_red\")))\n",
    "        .select(\"Description\")\n",
    "        .show(3, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date and timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dateDF = spark.range(10)\n",
    "                    .withColumn(\"today\", current_date())\n",
    "                    .withColumn(\"now\", current_timestamp())\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDF\n",
    ".select(col(\"today\"),\n",
    "date_sub(col(\"today\"), 5),\n",
    "date_add(col(\"today\"), 5))\n",
    ".show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDF\n",
    ".withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\n",
    ".select(datediff(col(\"week_ago\"), col(\"today\")))\n",
    ".show(1)\n",
    "dateDF\n",
    ".select(\n",
    "to_date(lit(\"2017-06-25\")).alias(\"start\"),\n",
    "to_date(lit(\"2018-11-21\")).alias(\"end\"))\n",
    ".select(months_between(col(\"start\"), col(\"end\")))\n",
    ".show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// WARNING SPARK COERCE DATES AND WHENEVER WE CANNOT PARSE THE DATE, HE WILL PUT NULL\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dateFormat = \"yyyy-dd-MM\"\n",
    "val cleanDateDF = spark.range(1).select(unix_timestamp(lit(\"2017-12-11\"), dateFormat).cast(\"timestamp\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with NULLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// fill na's by mapping a value for each column\n",
    "val fillColValues = Map(\n",
    "\"StockCode\" -> 5,\n",
    "\"Description\" -> \"No Value\"\n",
    ")\n",
    "df.na.fill(fillColValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.replace(\"Description\", Map(\"\" -> \"UNKNOWN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with complex types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//  structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// import org.apache.spark.sql.functions.struct\n",
    "val complexDF = df\n",
    ".select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(split(col(\"Description\"), \" \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// splitting an string on a data frame and then select and element from it\n",
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\n",
    ".selectExpr(\"array_col[2]\")\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// # explode\n",
    "\n",
    "// # The explode function takes a column that consists of arrays and creates one\n",
    "// # row (with the rest of the values duplicated) per value in the array. The\n",
    "// # following figure illustrates the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\n",
    ".withColumn(\"exploded\", explode(col(\"splitted\")))\n",
    ".select(\"Description\", \"InvoiceNo\", \"splitted\",\"exploded\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// maps\n",
    "\n",
    "df.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val jsonDF = spark.range(1)\n",
    ".selectExpr(\"\"\"\n",
    "'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF.select(\n",
    "get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\"),\n",
    "json_tuple(col(\"jsonString\"), \"myJSONKey\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val udfExampleDF = spark.range(5).toDF(\"num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power3(number:Double):Double = {\n",
    "        number * number * number\n",
    "    }\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val power3udf = udf(power3(_:Double):Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(power3udf(col(\"Quantity\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"power3\", power3(_:Double):Double)\n",
    "udfExampleDF.selectExpr(\"power3(num)\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
